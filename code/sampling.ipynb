{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sampling.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNK19FuTKU+lmLGc65cpsnm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xue1993/-federated_learning/blob/master/code/sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9v17SnN6GbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "# Python version: 3.6\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "def mnist_iid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample I.I.D. client data from MNIST dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return: dict of image index\n",
        "    \"\"\"\n",
        "    num_items = int(len(dataset)/num_users)\n",
        "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
        "    for i in range(num_users):\n",
        "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
        "                                             replace=False))\n",
        "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "def mnist_noniid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample non-I.I.D client data from MNIST dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # 60,000 training imgs -->  200 imgs/shard X 300 shards\n",
        "    num_shards, num_imgs = 200, 300\n",
        "    idx_shard = [i for i in range(num_shards)]\n",
        "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
        "    idxs = np.arange(num_shards*num_imgs)\n",
        "    labels = dataset.train_labels.numpy()\n",
        "\n",
        "    # sort labels\n",
        "    idxs_labels = np.vstack((idxs, labels))\n",
        "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
        "    idxs = idxs_labels[0, :]\n",
        "\n",
        "    # divide and assign 2 shards/client\n",
        "    for i in range(num_users):\n",
        "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
        "        idx_shard = list(set(idx_shard) - rand_set)\n",
        "        for rand in rand_set:\n",
        "            dict_users[i] = np.concatenate(\n",
        "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "def mnist_noniid_unequal(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample non-I.I.D client data from MNIST dataset s.t clients\n",
        "    have unequal amount of data\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :returns a dict of clients with each clients assigned certain\n",
        "    number of training imgs\n",
        "    \"\"\"\n",
        "    # 60,000 training imgs --> 50 imgs/shard X 1200 shards\n",
        "    num_shards, num_imgs = 1200, 50\n",
        "    idx_shard = [i for i in range(num_shards)]\n",
        "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
        "    idxs = np.arange(num_shards*num_imgs)\n",
        "    labels = dataset.train_labels.numpy()\n",
        "\n",
        "    # sort labels\n",
        "    idxs_labels = np.vstack((idxs, labels))\n",
        "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
        "    idxs = idxs_labels[0, :]\n",
        "\n",
        "    # Minimum and maximum shards assigned per client:\n",
        "    min_shard = 1\n",
        "    max_shard = 30\n",
        "\n",
        "    # Divide the shards into random chunks for every client\n",
        "    # s.t the sum of these chunks = num_shards\n",
        "    random_shard_size = np.random.randint(min_shard, max_shard+1,\n",
        "                                          size=num_users)\n",
        "    random_shard_size = np.around(random_shard_size /\n",
        "                                  sum(random_shard_size) * num_shards)\n",
        "    random_shard_size = random_shard_size.astype(int)\n",
        "\n",
        "    # Assign the shards randomly to each client\n",
        "    if sum(random_shard_size) > num_shards:\n",
        "\n",
        "        for i in range(num_users):\n",
        "            # First assign each client 1 shard to ensure every client has\n",
        "            # atleast one shard of data\n",
        "            rand_set = set(np.random.choice(idx_shard, 1, replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[i] = np.concatenate(\n",
        "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "\n",
        "        random_shard_size = random_shard_size-1\n",
        "\n",
        "        # Next, randomly assign the remaining shards\n",
        "        for i in range(num_users):\n",
        "            if len(idx_shard) == 0:\n",
        "                continue\n",
        "            shard_size = random_shard_size[i]\n",
        "            if shard_size > len(idx_shard):\n",
        "                shard_size = len(idx_shard)\n",
        "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
        "                                            replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[i] = np.concatenate(\n",
        "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "    else:\n",
        "\n",
        "        for i in range(num_users):\n",
        "            shard_size = random_shard_size[i]\n",
        "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
        "                                            replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[i] = np.concatenate(\n",
        "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "\n",
        "        if len(idx_shard) > 0:\n",
        "            # Add the leftover shards to the client with minimum images:\n",
        "            shard_size = len(idx_shard)\n",
        "            # Add the remaining shard to the client with lowest data\n",
        "            k = min(dict_users, key=lambda x: len(dict_users.get(x)))\n",
        "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
        "                                            replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[k] = np.concatenate(\n",
        "                    (dict_users[k], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "def cifar_iid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample I.I.D. client data from CIFAR10 dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return: dict of image index\n",
        "    \"\"\"\n",
        "    num_items = int(len(dataset)/num_users)\n",
        "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
        "    for i in range(num_users):\n",
        "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
        "                                             replace=False))\n",
        "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "def cifar_noniid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample non-I.I.D client data from CIFAR10 dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    num_shards, num_imgs = 200, 250\n",
        "    idx_shard = [i for i in range(num_shards)]\n",
        "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
        "    idxs = np.arange(num_shards*num_imgs)\n",
        "    # labels = dataset.train_labels.numpy()\n",
        "    labels = np.array(dataset.train_labels)\n",
        "\n",
        "    # sort labels\n",
        "    idxs_labels = np.vstack((idxs, labels))\n",
        "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
        "    idxs = idxs_labels[0, :]\n",
        "\n",
        "    # divide and assign\n",
        "    for i in range(num_users):\n",
        "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
        "        idx_shard = list(set(idx_shard) - rand_set)\n",
        "        for rand in rand_set:\n",
        "            dict_users[i] = np.concatenate(\n",
        "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True,\n",
        "                                   transform=transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,),\n",
        "                                                            (0.3081,))\n",
        "                                   ]))\n",
        "    num = 100\n",
        "    d = mnist_noniid(dataset_train, num)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}