{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO53aEgIy36uyawehnAQSCe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xue1993/-federated_learning/blob/master/code/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHUqJ5Na6bRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "# Python version: 3.6\n",
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer_input = nn.Linear(dim_in, dim_hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.layer_hidden = nn.Linear(dim_hidden, dim_out)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, x.shape[1]*x.shape[-2]*x.shape[-1])\n",
        "        x = self.layer_input(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_hidden(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "\n",
        "class CNNMnist(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNNMnist, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, args.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class CNNFashion_Mnist(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNNFashion_Mnist, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.fc = nn.Linear(7*7*32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CNNCifar(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNNCifar, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, args.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class modelC(nn.Module):\n",
        "    def __init__(self, input_size, n_classes=10, **kwargs):\n",
        "        super(AllConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_size, 96, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(96, 96, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(96, 96, 3, padding=1, stride=2)\n",
        "        self.conv4 = nn.Conv2d(96, 192, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(192, 192, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(192, 192, 3, padding=1, stride=2)\n",
        "        self.conv7 = nn.Conv2d(192, 192, 3, padding=1)\n",
        "        self.conv8 = nn.Conv2d(192, 192, 1)\n",
        "\n",
        "        self.class_conv = nn.Conv2d(192, n_classes, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_drop = F.dropout(x, .2)\n",
        "        conv1_out = F.relu(self.conv1(x_drop))\n",
        "        conv2_out = F.relu(self.conv2(conv1_out))\n",
        "        conv3_out = F.relu(self.conv3(conv2_out))\n",
        "        conv3_out_drop = F.dropout(conv3_out, .5)\n",
        "        conv4_out = F.relu(self.conv4(conv3_out_drop))\n",
        "        conv5_out = F.relu(self.conv5(conv4_out))\n",
        "        conv6_out = F.relu(self.conv6(conv5_out))\n",
        "        conv6_out_drop = F.dropout(conv6_out, .5)\n",
        "        conv7_out = F.relu(self.conv7(conv6_out_drop))\n",
        "        conv8_out = F.relu(self.conv8(conv7_out))\n",
        "\n",
        "        class_out = F.relu(self.class_conv(conv8_out))\n",
        "        pool_out = F.adaptive_avg_pool2d(class_out, 1)\n",
        "        pool_out.squeeze_(-1)\n",
        "        pool_out.squeeze_(-1)\n",
        "        return pool_out\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}