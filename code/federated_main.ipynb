{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "federated_main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xue1993/-federated_learning/blob/master/code/federated_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMpvM1yv5CCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "# Python version: 3.6\n",
        "\n",
        "!pip install import-ipynb\n",
        "import import_ipynb\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "try:\n",
        "    from tensorboardX import SummaryWriter\n",
        "except:\n",
        "    os.system('pip install tensorboardX')\n",
        "    from tensorboardX import SummaryWriter \n",
        "\n",
        "from options import args_parser\n",
        "from update import LocalUpdate, test_inference \n",
        "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
        "from utils import get_dataset, average_weights, exp_details\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # define paths\n",
        "    pathlogs = './logs'\n",
        "    if not os.path.exists(pathlogs):\n",
        "        os.makedirs(pathlogs)\n",
        "    logger = SummaryWriter(pathlogs)\n",
        "\n",
        "    args = args_parser()\n",
        "    \n",
        "    if args.savetxt:\n",
        "        print('output is in the outputtxt')\n",
        "        orig_stout = sys.stdout\n",
        "        ftxt = open('out.txt','a')\n",
        "        sys.stdout = ftxt\n",
        "        \n",
        "\n",
        "\n",
        "    exp_details(args)\n",
        "    \n",
        "    sparsityrate = args.spr\n",
        "\n",
        "    if args.gpu:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "    device = 'cuda' if args.gpu else 'cpu'\n",
        "    \n",
        "    \n",
        "    \n",
        "    # load dataset and user groups\n",
        "    train_dataset, test_dataset, user_groups = get_dataset(args)\n",
        "\n",
        "    # BUILD MODEL\n",
        "    if args.model == 'cnn':\n",
        "        # Convolutional neural netork\n",
        "        if args.dataset == 'mnist':\n",
        "            global_model = CNNMnist(args=args)\n",
        "        elif args.dataset == 'fmnist':\n",
        "            global_model = CNNFashion_Mnist(args=args)\n",
        "        elif args.dataset == 'cifar':\n",
        "            global_model = CNNCifar(args=args)\n",
        "\n",
        "    elif args.model == 'mlp':\n",
        "        # Multi-layer preceptron\n",
        "        img_size = train_dataset[0][0].shape\n",
        "        len_in = 1\n",
        "        for x in img_size:\n",
        "            len_in *= x\n",
        "            global_model = MLP(dim_in=len_in, dim_hidden=64,\n",
        "                               dim_out=args.num_classes)\n",
        "    else:\n",
        "        exit('Error: unrecognized model')\n",
        "\n",
        "    # Set the model to train and send it to device.\n",
        "    global_model.to(device)\n",
        "    global_model.train()\n",
        "    print(global_model)\n",
        "\n",
        "    # copy weights\n",
        "    global_weights = global_model.state_dict()\n",
        "\n",
        "    # Training\n",
        "    train_loss, train_accuracy = [], []\n",
        "    val_acc_list, net_list = [], []\n",
        "    cv_loss, cv_acc = [], []\n",
        "    print_every = 1\n",
        "    val_loss_pre, counter = 0, 0\n",
        "\n",
        "    for epoch in tqdm(range(args.epochs)):\n",
        "        local_weights, local_losses = [], []\n",
        "        print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
        "\n",
        "        global_model.train()\n",
        "        m = max(int(args.frac * args.num_users), 1)\n",
        "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
        "\n",
        "        for idx in idxs_users:\n",
        "            local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
        "                                      idxs=user_groups[idx], logger=logger)\n",
        "            w, loss = local_model.update_weights(\n",
        "                model=copy.deepcopy(global_model), global_round=epoch)\n",
        "            for key in w.keys(): \n",
        "                mask = torch.rand(*w[key].shape)\n",
        "                w[key][mask<sparsityrate] = global_weights[key][mask<sparsityrate]\n",
        "            local_weights.append(copy.deepcopy(w))\n",
        "            local_losses.append(copy.deepcopy(loss))\n",
        "            \n",
        "    \n",
        "        # update global weights\n",
        "        global_weights = average_weights(local_weights)\n",
        "\n",
        "        # update global weights\n",
        "        global_model.load_state_dict(global_weights)\n",
        "\n",
        "        loss_avg = sum(local_losses) / len(local_losses)\n",
        "        train_loss.append(loss_avg)\n",
        "\n",
        "        # Calculate avg training accuracy over all users at every epoch\n",
        "        list_acc, list_loss = [], []\n",
        "        global_model.eval()\n",
        "        for c in range(args.num_users):\n",
        "            local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
        "                                      idxs=user_groups[idx], logger=logger)\n",
        "            acc, loss = local_model.inference(model=global_model)\n",
        "            list_acc.append(acc)\n",
        "            list_loss.append(loss)\n",
        "        train_accuracy.append(sum(list_acc)/len(list_acc))\n",
        "\n",
        "        # print global training loss after every 'i' rounds\n",
        "        if (epoch+1) % print_every == 0:\n",
        "            print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
        "            print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
        "            print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
        "\n",
        "    # Test inference after completion of training\n",
        "    test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
        "\n",
        "    print(f' \\n Results after {args.epochs} global rounds of training:')\n",
        "    print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
        "    print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
        "\n",
        "    if args.savetxt:\n",
        "        sys.stdout = orig_stout\n",
        "        ftxt.close()\n",
        "        print('outputtxt is closed')\n",
        "        \n",
        "        \n",
        "    # Saving the objects train_loss and train_accuracy:\n",
        "    #file_name = './save/objects/{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}].pkl'.\\\n",
        "    #    format(args.dataset, args.model, args.epochs, args.frac, args.iid,\n",
        "    #           args.local_ep, args.local_bs)\n",
        "\n",
        "    #with open(file_name, 'a') as f:\n",
        "    #    pickle.dump([train_loss, train_accuracy], f)\n",
        "\n",
        "    print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
        "    \n",
        "    \n",
        "\n",
        "    # PLOTTING (optional)\n",
        "    # import matplotlib\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # matplotlib.use('Agg')\n",
        "\n",
        "    # Plot Loss curve\n",
        "    # plt.figure()\n",
        "    # plt.title('Training Loss vs Communication rounds')\n",
        "    # plt.plot(range(len(train_loss)), train_loss, color='r')\n",
        "    # plt.ylabel('Training loss')\n",
        "    # plt.xlabel('Communication Rounds')\n",
        "    # plt.savefig('./save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
        "    #             format(args.dataset, args.model, args.epochs, args.frac,\n",
        "    #                    args.iid, args.local_ep, args.local_bs))\n",
        "    #\n",
        "    # # Plot Average Accuracy vs Communication rounds\n",
        "    # plt.figure()\n",
        "    # plt.title('Average Accuracy vs Communication rounds')\n",
        "    # plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
        "    # plt.ylabel('Average Accuracy')\n",
        "    # plt.xlabel('Communication Rounds')\n",
        "    # plt.savefig('./save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
        "    #             format(args.dataset, args.model, args.epochs, args.frac,\n",
        "    #                    args.iid, args.local_ep, args.local_bs))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}